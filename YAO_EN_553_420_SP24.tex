\documentclass[titlepage]{article}

\usepackage{preamble}

\begin{document}

\maketitle

\tableofcontents

\newpage \section{Introduction.}

\subsection{Introduction.} Introduction.

\subsection{Remark.} These notes follow the material presented in EN.553.420 Introduction to Probability taught by Professor Fred Torcaso during the semester of Spring 2024 at The Johns Hopkins University. These notes shall be brief as they are meant to supplement the lectures, homeworks, and notes provided by Professor Torcaso.

\newpage \section{Probability Spaces.}

\subsection{Introduction} Probabilty Spaces.

\subsection{Remark.} The following definitions shall be informal. For a more formal treatment of probability, please refer to the EN.553.430 Mathematical Statistics.

\subsection{Remark.} In this class, we shall investigate three notions of probabiliy:
\begin{enumerate}
\item[(1)] Classical probability.
\item[(2)] Discrete probability.
\item[(3)] Continuous probability.
\end{enumerate}
We shall see that each of these frameworks satisfies the below axioms.

\subsection{Definition.} A sample space is a set denoted by the symbol $\Omega$.

\subsection{Definition.} The event space $\mathcal{F}$ is a collection of subsets of $\Omega$. An event is denoted by the symbol $A$. (Formally, the event space $\mathcal{F}$ is a sigma-algebra.)

\subsection{Definition.} A probability measure is a function $P: \mathcal{F} \to \mathbb{R}$ subject to the three Komolgorov Axioms:
\begin{enumerate}
\item[(1)] $\forall A \in \mathcal{F}, P(A) \geq 0.$ (Nonnegativity.)
\item[(2)] $P(\Omega) = 1.$ (Normaliziation.)
\item[(3)] If $A_{1}, A_{2}, \ldots \subseteq \Omega$ is a countable sequence of disjoint events, i.e, for any $i \neq j, A_{i} \cap A_{j} = \emptyset$, then 
$$P\left(\bigcup_{i=1}^{\infty}A_{i}\right) = \sum_{i=1}^{n}P(A_{i}).$$
(Countable additivity.)
\end{enumerate}

\subsection{Definition.} A probability space is an ordered triple $(\Omega, \mathcal{F}, P)$.

\subsection{Theorem.} From the Komolgorov axioms, we find 
that the probability of the empty set is 
$$P(\emptyset) = 0$$
and that if $A_{1}, \ldots, A_{n} \subseteq \Omega$ is a finite sequence of disjoint events, then 
$$P\left(\bigcup_{i=1}^{n}A_{i}\right) = \sum_{i=1}^{n}P(A_{i})$$
and that for any two events $A, B \subseteq \Omega$, 
$$P(A \cup B) = P(A) + P(B) - P(A \cap B).$$

\subsection{Proof.} (Continued.) The proof is presented in lecture. The reproduction of the proof is left as an exercise to the student.

\subsection{Theorem.} (Inclusion-exclusion principle.) Suppose that $A_{1}, \ldots, A_{n} \subseteq \Omega$ is a finite sequence of events. The probability of the union is the sum of all of the probabilities $i$-intersections with alternating signs. The notation is cumbersome, so the example for $n = 3$ is 
$$P(A \cup B \cup C) = P(A) + P(B) + P(C) - P(A \cap B) - P(A \cap C) - P(B \cap C) + P(A \cap B \cap C).$$

\subsection{Proof.} (Continued.) The proof is by induction.

\newpage \section{Combinatorics.}

\subsection{Combinatorics.} Combinatorics.

\subsection{Definition.} If $n \in \mathbb{N}$, then then the factorial is defined as 
$$n! = n(n-1)\ldots1.$$
If $n = 0$, then the empty product is defined as 
$$0! = 1.$$

\subsection{Definition.} Suppose that we have $n$ objects from which we want to choose an ordered permutation of $k$ objects. Then, the number of permutations is given by the falling factorial 
$$(n)_{k} = \frac{n!}{(n-k)!}.$$

\subsection{Definition.} Suppose that we have $n$ objects from which we want to choose an unordered subset of $k$ objects. Then, the number of subsets is given by the binomial coefficient 
$$\binom{n}{k} = \frac{n!}{k!(n-k)!},$$
which is pronounced '$n$ choose $k$'. Observe that 
$$\binom{n}{k} = \binom{n}{n-k}.$$

\subsection{Theorem.} (Pascal's identity.) 
$$\binom{n}{k} = \binom{n-1}{k-1} + \binom{n-1}{k}.$$

\subsection{Proof.} (Continued.) The proof is left as an exercise to the student.

\subsection{Theorem.} (Binomial theorem.) If $a, b \in \mathbb{R}$ and $n = 1, 2, \ldots$, then 
$$(a + b)^{n} = \sum_{k=1}^{n}\binom{n}{k}a^{n}b^{n-k}.$$

\subsection{Corollary.} (Continued.)
$$\sum_{k=1}^{n}\binom{n}{k} = 2^{n}.$$
We may interpret this corollary as that if $A$ is a set with $n$ elements, then the power set, i.e., the set of all subsets, of $A$ has $2^{n}$ elements.

\subsection{Example.} The number of anagrams of $PEEPEE$ is 
$$\binom{6}{2} = \frac{6!}{2!4!}$$
because we choose $2$ of the positions to be $P$ and the rest to be $E$. Alternatively, we may realize that there are $6!$ ways to arrange the six letters and that within each group, there are $2!$ and $4!$ ways, respectively, to arrange the letters within a group such that the result is identical.

\subsection{Definition.} If $n, k_{1}, \ldots, k_{K} \in \mathbb{N}$ and $k_{1} + \ldots + k_{K} = n$, then the multinomial coefficient is defined 
$$\binom{n}{k_{1},\ldots,k_{K}} = \frac{n!}{k_{1}!\ldots k_{K}!}.$$
If $k_{1} + \ldots k_{K} < n$, then we take that 
$$\binom{n}{k_{1},\ldots,k_{K}} = \binom{n}{k_{1},\ldots,k_{K},n-k_{1}-\ldots-k_{K}}.$$
To be clear, however, it is good practice to ensure that $k_{1} + \ldots + k_{K} = n$.

\subsection{Example.} The number of anagrams of $PEEPEEPOOPOO$ is 
$$\binom{12}{4,4,4} = \frac{12!}{4!4!4!}$$
with similar reasoning as with example 2.10.

\subsection{Definition.} A multiset is a collection of objects where objects may be repeated, i.e., the objects have a multiplicity. A multiset is denoted with curly braces, e.g.,  
$$\{A, B, B, \ldots\}.$$

\subsection{Theorem.} (Stars and bars.) Suppose that we have $r$ distinguishable bins into which we want to place $n$ indistinguishable objects. Then, the number of ways to do so is given by 
$$\binom{n+r-1}{n} = \binom{n+r-1}{r-1}.$$

\subsection{Proof.} (Continued.) We enumerate the bins $i = 1, \ldots, r$. To each bin $i$, we want to assign $n_{i}$ balls such that $n_{1} + \ldots + n_{r} = n$. But the ways to do so are in bijection with the anagrams of $n$ stars $\star$ and $k$ bars $\vert$ where the stars denote the balls and the bars denote the boundaries between the bins. We realize that there are 
$$\binom{n+r-1}{n} = \binom{n+r-1}{r-1}$$
such anagrams.

\subsection{Corollary.} (Continued.) The number of multisets of $n$ elements with $r$ distinct labels allowed is 
$$\binom{n+r-1}{n} = \binom{n+r-1}{r-1}.$$

\subsection{Corollary.} (Continued.) The number of integer $r$-partitions of $n$, i.e., the number of integer solutions to $n_{1} + \ldots + n_{r} = n$ is 
$$\binom{n+r-1}{n} = \binom{n+r-1}{r-1}.$$

\subsection{Lemma.} Suppose that we have a two-step experiment. There are $m$ ways to perform the first step. For each of those ways, there are $n$ ways to perform the second step. Then, there are $mn$ ways to perform the experiment in total.

\newpage \section{Classical Probability.}

\subsection{Introduction.} Classical Probability.

\subsection{Definition.} Suppose that $\Omega$ is a finite set and $A \subseteq \Omega$. Furthermore, suppose that every singleton subset of $\Omega$ is equiprobable. Then, 
$$P(A) = \frac{|\Omega|}{|A|}$$
where $|\cdot|$ denotes the cardinality of the set $\cdot$.

\subsection{Remark.} Since every singleton event is equiprobable, then we may determine the probability of a composite event by counting the cardinality of that event. Classical probability is thus combinatorial.

\subsection{Example.} Suppose that we have a standard 52-card deck from which we select a hand of five cards. The probability that we have a full house is 
$$(13)_{2}\binom{4}{3}\binom{4}{2} / \binom{52}{5} \approx 0.001441$$
because we select an ordered pair of ranks (one rank for the three-of-a-kind and one rank for the pair) and then select the suits for the three-of-a-kind and the pair.

\newpage \newsection{Conditional Probability.}

\subsection{Introduction.} Conditional Probability.

\subsection{Definition.} For events $A, B \subseteq \Omega$, the conditional probability of $A$ given $B$ is 
$$P(A \mid B) = \frac{P(A \cap B)}{P(B)}$$
provided that $P(B) > 0$.

\textbf{Theorem.} (Law of Total Probability) Let $B_{1}, B_{2}, \ldots$ be a partition of $\Omega$. For any event $A \subseteq \Omega$, we have that 
$$P(A) = \sum_{i=1}^{\infty}P(A \mid B_{i})P(B_{i}).$$

\textbf{Example.} Suppose that we have two coins, one fair and one with only heads. We select a coin at random and flip it $n$ times. Find the probability that we observe all heads.

Let $A$ be the event that we select all heads and let $B, B^{c}$ be the events that we select the unfair coin and teh fair coin, respectively. We have that 
\begin{align*}
    P(A) &= P(A \mid B)P(B) + P(A \mid B^{c})P(B^{c}) \\
         &= 1/2 + (1/2)^{n+1}.
\end{align*}

\subsection{Theorem.} (Bayes's Rule) For events $A, B \subseteq \Omega$, the conditional probability of $A$ given $B$ is 
$$P(A \mid B) = \frac{P(B \mid A)P(A)}{P(B)}.$$

\subsection{Proof.} (Continued.) Observe that 
\begin{align*}
    P(A \mid B) &= \frac{P(A \cap B)}{P(B)} \\
                &= \frac{P(B \mid A)P(A)}{P(B)}.
\end{align*}

\subsection{Example.} Suppose that we have two coins, one fair and one with only heads. We select a coin at random and flip it $n$ times, observing all heads. Find the minimum number of heads to conclude that the coin is the unfair coin with probability greater than $p$.

Let $A$ be the event that we select the unfair coin and $B$ be the event that we observe all heads. By Bayes's rule, we have that  
\begin{align*}
    P(A \mid B) &= \frac{P(B \mid A)P(A)}{P(B)} \\
                &= \frac{P(B \mid A)P(A)}{P(B \mid A)P(A) + P(B \mid A^{c} \mid A^{c})} \\
                &= \frac{1/2}{1/2 + (1/2)^{n+1}} \\
                &= \frac{1}{1 + (1/2)^{n}}.
\end{align*}
If we want to solve for $n$, then we have that 
\begin{align*}
    p &<\frac{1}{1 + (1/2)^{n}} \\
    2^{-n} &< \frac{1}{p} - 1,
\end{align*}
which yields our final answer of 
$$n > \log_{2}\left(\frac{p}{1-p}\right).$$

\textbf{Definition.} Two events $A, B \subseteq \Omega$ are independent if 
$$P(A) = P(A \mid B).$$
Equivalently, 
$$P(A \cap B) = P(A)P(B).$$ Of course, $A$ is independent of $B$ if and only if $B$ is independent of $A$.

\textbf{Definition.} Let $A_{1}, A_{2}, \ldots \subseteq \Omega$ be a sequence of events. The events are independent if for any index set $I \subseteq \mathbb{N}$, we have that
$$P\left(\bigcap_{i \in I}A_{i}\right) = \prod_{i \in I}P(A_{i}).$$

\textbf{Theorem.} If $A_{1}, A_{2}, \ldots \subseteq \Omega$ are independent, then replacing any event $A_{i}$ with its complement $A_{i}^{c}$ yields a sequence of independent events.

\newpage \newsection{Random Variables.}

\subsection{Introduction.} Random Variables.

\subsection{Definition.} A random variable is a (measurable) function $X: \Omega \to \mathbb{R}$.

\subsection{Definition.} The cumulative distribution function (cdf) of a random variable $X$ is a function $F: \mathbb{R} \to \mathbb{R}$ defined by
$$F(x) = P(X \leq x).$$
A cdf has the properties 
\begin{enumerate}
\item[(1)] $\lim_{x \to -\infty}F(x) = 0$.
\item[(2)] $\lim_{x \to \infty}F(x) = 1$.
\item[(3)] $F$ is nondecreasing.
\item[(4)] $F$ is right-continuous.
\end{enumerate}

\subsection{Definition.} A discrete random variable $X$ is a (measurable) function $X: \Omega \to \text{supp}(X)$ where $\text{supp}(X)$ is a countable set. We shall furthermore take $\text{supp}(X) \subseteq \mathbb{Z}$.

\subsection{Definition.} The probability mass function of a discrete rv $X$ is a probability measure $p: \mathbb{Z} \to \mathbb{R}$. In particular if a random variabe $X$ possesses pmf $p$, then the probability of event $A \subseteq \Omega$ is
$$P(X \in A) = \sum_{x \in A}p(x).$$

\subsection{Definition.} A continuous random variable $X$ is a (measurable) function $X: \Omega \to \mathbb{R}$ with a continuous cdf $F: \mathbb{R} \to \mathbb{R}$.

\subsection{Definition.} If $X$ is absolutely continuous, then there exists a probability density function (pdf) $f: \mathbb{R} \to \mathbb{R}$. In particular if a random variabe $X$ possesses pdf $f$, then the probability of event $A \subseteq \Omega$ is 
$$P(X \in A) = \int_{A}f(x)dx.$$

\subsection{Definition.} The support of a discrete rv $X$ with pmf $p$ is  
$$\text{supp}(p) = \{x \in \mathbb{Z} : p(x) > 0\}.$$
The support of a continuous rv $X$ with pdf $f$ is
$$\text{supp}(f) = \{x \in \mathbb{R} : f(x) > 0\}.$$
An equivalent notation for either is $\text{supp}(X)$.

\subsection{Remark.} The following definitions sum over $\mathbb{Z}$ or integrate of $\mathbb{R}$. Since the pmf or pdf is zero off of the support, then we may equivalently sum or integrate over the support.

\subsection{Definition.} The expectation of a discrete rv $X$ with pmf $p$
$$\E(X) = \sum_{i=-\infty}^{\infty}xp(x).$$
The expectation of a continuous rv $X$ with pdf $f$
$$\E(X) = \int_{-\infty}^{\infty}xf(x)dx.$$
We sometimes denote $\mu = \E(X)$.

\subsection{Theorem.} (Law of the unconscious statistician.) Let $g: \mathbb{R} \to \mathbb{R}$ be a function. Let $X$ be a discrete rv with pmf $p$. The expectation of $g(X)$ is 
$$\E(g(X)) = \sum_{i=-\infty}^{\infty}g(x)p(x).$$
If instead $X$ is a continuous rv with pdf $f$, then the expectation of $f(X)$ is
$$\E(g(X)) = \int_{-\infty}^{\infty}g(x)f(x)dx.$$

\subsection{Theorem.} Expectation is a linear operator, i.e., for two random variables $X, Y$ and $a, b \in \mathbb{R}$, we have that 
$$\E(aX + bY) = a\E(X) + b\E(Y).$$

\subsection{Definition.} Let $k = 1, 2, \ldots$. The $k$-th moment of an rv $X$ is 
$$\E(X^{k}).$$

\subsection{Definition.} Let $k = 1, 2, \ldots$. The $k$-th moment central moment of an rv $X$ is 
$$\E((X-\mu)^{k}).$$

\subsection{Definition.} The variance of an rv $X$ is the second central moment 
\begin{align*}
    \Var(X) &= \E((X-\mu)^{2}) \\
            &= \E(X^{2}) - \E(X)^{2}.
\end{align*}

\subsection{Definition.} The skewness of an rv $X$ is the third central moment 
$$\E((X-\mu)^{3}).$$

\subsection{Definition.} The kurtosis ('tailedness') of an rv $X$ is the fourth central moment 
$$\E((X-\mu)^{4}).$$
The excess kurtosis is $\E((X-\mu)^{4}) - 3$ (because the kurtosis of the normal distribution is $3$).

\subsection{Definition.} The moment generating function of an rv $X$ is 
$$M(\theta) = \E(e^{\theta X})$$
whenever the expectation exists on an open interval of $\theta$ around $0$. If the mgf exists, then the $k$th moment of $X$ is equal to 
$$\E(X^{k}) = M^{(k)}(0).$$

\subsection{Theorem.} If the moment generating function of two rvs $X$ and $Y$ are equal, then $X$ and $Y$ have the same distribution.

\subsection{Lemma.} Suppose that $X$ has mgf $M_{X}(\theta)$. Then the mgf of $Y = aX + b$ is
$$M_{Y}(\theta) = e^{b\theta}M_{X}(a\theta).$$

\subsection{Proof.} (Continued.) The proof is left as an exercise to the student.

\subsection{Definition.} Common distributions are found in the formula sheet \href{https://github.com/dyao13/EN_553_420_SP24/blob/main/420distribution_table.pdf}{\underline{here}}.

\subsection{Remark.} Important interpretations of common discrete distributions include the following.
\begin{enumerate}
\item[(1)] The Bernoulli distribution models the success of a single trial.
\item[(2)] The binomial distribution models the number of successes in $n$ independent Bernoulli trials.
\item[(3)] The Poisson distribution models the number of events in a fixed interval of time subject to the assumptions of a Poisson process: (1) events occur independently, (2) events occur at a constant rate.
\item[(4)] The geometric distribution models the number of independent Bernoulli trials until the first success.
\item[(5)] The negative binomial distribution models the number of independent Bernoulli trials until the $r$-th success.
\item[(6)] The hypergeometric distribution models the number of successes in $n$ draws without replacement from a finite population.
\item[(7)] The discrete uniform distribution models the outcomes of a finite set of equally likely outcomes.
\item[(8)] The multinomial distribution models the number of successes in $n$ independent trials with $K$ possible outcomes.
\end{enumerate}

\subsection{Remark.} Important interpretations of common continuous distributions include the following.
\begin{enumerate}
\item[(1)] The uniform distribution models the outcomes of a continuous set of equally likely outcomes.
\item[(2)] The exponential distribution models the time until the first event in a Poisson process.
\item[(3)] The Gamma distribution models the time until the $r$-th event in a Poisson process.
\item[(4)] The normal distribution models the sum of a large number of independent and identically distributed random variables.
\item[(5)] The log-normal distribution models the exponential of a normal random variable.
\item[(7)] The beta distribution models $r$th largest of $n$ independent uniform random variables.
\end{enumerate}

\newpage \newsection{Joint Distributions.}

\subsection{Introduction.} Joint Distributions.

\newpage \newsection{Transformations of Random Variables.}

\subsection{Introduction.} Transformations of Random Variables.

\newpage \newsection{Order Statistics.}

\subsection{Introduction.} Order Statistics.

\newpage \newsection{Convergences and Inequalities.}

\subsection{Introduction.} Convergences and Inequalities.

\end{document}